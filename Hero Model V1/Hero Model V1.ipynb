{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Natural Processing Language Model for Healthcare Assistant Chat Bot - Multiclassification***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook contains a model for predicting medical condition categories based on user inputted statements. Here is the basic structure of the code to help guide the user through the different stages of Machine Learning... enjoy! \n",
    "\n",
    "- Stage 1, Pre-processing (Exploration, Cleaning, Preprocessing, Embedding, Merging)\n",
    "- Stage 2, Training (Defining Target Variables and Input Values(X and Y), Splitting Training and Testing Samples, Identification of Classes and Parameters, Fitting model architecture)\n",
    "- Stage 3, Testing and Evaluation (Model Accuracy, Epoch statistics)\n",
    "- Stage 4, Optimisation (Feature Engineering, Hyperparemeter Tuning)\n",
    "- Stage 5, Saving and Loading (Model Completion, Packaging)\n",
    "- Stage 6, Inference Testing (Testing new samples and input data structure)\n",
    "\n",
    "This strucure is LOOSE as we return to earlier stages to optimise our dataset and model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 1 - Exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                question  \\\n",
      "10547  What are the genetic changes related to leukoe...   \n",
      "15995         What to do for Primary Biliary Cirrhosis ?   \n",
      "15841           Who is at risk for Fecal Incontinence? ?   \n",
      "8994   What is (are) Pervasive Developmental Disorders ?   \n",
      "2518           What are the symptoms of Crome syndrome ?   \n",
      "\n",
      "                                                  answer source  \\\n",
      "10547  LBSL is caused by mutations in the DARS2 gene,...    GHR   \n",
      "15995  A healthy diet is important in all stages of c...  NIDDK   \n",
      "15841  Nearly 18 million U.S. adultsabout one in 12ha...  NIDDK   \n",
      "8994   The diagnostic category of pervasive developme...  NINDS   \n",
      "2518   What are the signs and symptoms of Crome syndr...   GARD   \n",
      "\n",
      "                                              focus_area  \n",
      "10547  leukoencephalopathy with brainstem and spinal ...  \n",
      "15995                          Primary Biliary Cirrhosis  \n",
      "15841                                 Fecal Incontinence  \n",
      "8994                   Pervasive Developmental Disorders  \n",
      "2518                                      Crome syndrome  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"herodataset.csv\", encoding=\"ISO-8859-1\")\n",
    "data = df.sample(n=1000, random_state=42) \n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing pandas and the dataset. Printing the first 5 samples to check correct import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source</th>\n",
       "      <th>focus_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>986</td>\n",
       "      <td>973</td>\n",
       "      <td>9</td>\n",
       "      <td>854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>What are the treatments for Colorectal Cancer ?</td>\n",
       "      <td>This condition is inherited in an autosomal re...</td>\n",
       "      <td>GARD</td>\n",
       "      <td>Colorectal Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>337</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "count                                              1000   \n",
       "unique                                              986   \n",
       "top     What are the treatments for Colorectal Cancer ?   \n",
       "freq                                                  3   \n",
       "\n",
       "                                                   answer source  \\\n",
       "count                                                1000   1000   \n",
       "unique                                                973      9   \n",
       "top     This condition is inherited in an autosomal re...   GARD   \n",
       "freq                                                   22    337   \n",
       "\n",
       "               focus_area  \n",
       "count                 997  \n",
       "unique                854  \n",
       "top     Colorectal Cancer  \n",
       "freq                    6  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source</th>\n",
       "      <th>focus_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>986</td>\n",
       "      <td>973</td>\n",
       "      <td>9</td>\n",
       "      <td>854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>What are the treatments for Colorectal Cancer ?</td>\n",
       "      <td>This condition is inherited in an autosomal re...</td>\n",
       "      <td>GARD</td>\n",
       "      <td>Colorectal Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>337</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "count                                              1000   \n",
       "unique                                              986   \n",
       "top     What are the treatments for Colorectal Cancer ?   \n",
       "freq                                                  3   \n",
       "\n",
       "                                                   answer source  \\\n",
       "count                                                1000   1000   \n",
       "unique                                                973      9   \n",
       "top     This condition is inherited in an autosomal re...   GARD   \n",
       "freq                                                   22    337   \n",
       "\n",
       "               focus_area  \n",
       "count                 997  \n",
       "unique                854  \n",
       "top     Colorectal Cancer  \n",
       "freq                    6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the sample sizes in the dataset, as this is a huge dataset, it would be better to test with 1000 samples first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 854 unique focus areas, as this will be our y label the model is predicting, we will need to reduce this number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems Cancer is the overwhelming number in this model, this could lead to bias. For future engineering we should maybe reduce this size comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 1 - Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate Answers:\n",
      "answer\n",
      "This condition is inherited in an autosomal recessive pattern, which means both copies of the gene in each cell have mutations. The parents of an individual with an autosomal recessive condition each carry one copy of the mutated gene, but they typically do not show signs and symptoms of the condition.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                22\n",
      "#NAME?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          3\n",
      "This condition is inherited in an autosomal dominant pattern, which means one copy of the altered gene in each cell is sufficient to cause the disorder.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        3\n",
      "On this Page General Information What is vancomycin-resistant enterococci? What types of infections does vancomycin-resistant enterococci cause? Are certain people at risk of getting vancomycin-resistant enterococci? What is the treatment for vancomycin-resistant enterococci? How is vancomycin-resistant enterococci spread? How can patients prevent the spread of vancomycin-resistant enterococci? What should a patient do if they think they have vancomycin-resistant enterococci? Recommendations and Guidelines General Information For more images of this bacterium, search the Public Health Image Library What is vancomycin-resistant enterococci? Enteroccocci are bacteria that are normally present in the human intestines and in the female genital tract and are often found in the environment. These bacteria can sometimes cause infections. Vancomycin is an antibiotic that is used to treat some drug-resistant infections caused by enterococci. In some instances, enterococci have become resistant to this drug and thus are called vancomycin-resistant enterococci (VRE). Most VRE infections occur in hospitals. Top of page What types of infections does VRE cause? VRE can live in the human intestines and female genital tract without causing disease (often called colonization). However, sometimes it can cause infections of the urinary tract, the bloodstream, or of wounds associated with catheters or surgical procedures. Top of page Are certain people at risk of getting VRE? The following persons are at increased risk becoming infected with VRE: People who have been previously treated with the antibiotic vancomycin or other antibiotics for long periods of time. People who are hospitalized, particularly when they receive antibiotic treatment for long periods of time. People with weakened immune systems such as patients in intensive care units, or in cancer or transplant wards. People who have undergone surgical procedures such as abdominal or chest surgery. People with medical devices that stay in for some time such as urinary catheters or central intravenous (IV) catheters. People who are colonized with VRE. Top of page What is the treatment for VRE? People with colonized VRE (bacteria are present, but have no symptoms of an infection)  do not need treatment. Most VRE infections can be treated with antibiotics other than vancomycin. Laboratory testing of the VRE can determine which antibiotics will work. For people who get VRE infections in their bladder and have urinary catheters, removal of the catheter when it is no longer needed can also help get rid of the infection. Top of page How is VRE spread? VRE is often passed from person to person by the contaminated hands of caregivers. VRE can get onto a caregiver's hands after they have contact with other people with VRE or after contact with contaminated surfaces. VRE can also be spread directly to people after they touch surfaces that are contaminated with VRE. VRE is not spread through the air by coughing or sneezing. Top of page How can patients prevent the spread of VRE? If a patient or someone in their household has VRE, the following are some things they can do to prevent the spread of VRE: Keep their hands clean. Always wash their hands thoroughly after using the bathroom and before preparing food. Clean their hands after contact with persons who have VRE. Wash with soap and water (particularly when visibly soiled) or use alcohol-based hand rubs. Frequently clean areas of the home, such as bathrooms, that may become contaminated with VRE. Wear gloves if hands may come in contact with body fluids that may contain VRE, such as stool or bandages from infected wounds. Always wash their hands after removing gloves. If someone has VRE, be sure to tell healthcare providers so that they are aware of the infection. Healthcare facilities use special precautions to help prevent the spread of VRE to others. Top of page What should patients do if they think they have vancomycin-resistant enterococci (VRE)? Anyone who thinks they have VRE must talk with their healthcare provider. Top of page Recommendations and Guidelines For more information about prevention and treatment of HAIs, see the resources below: Siegel JD, Rhinehart E, Jackson M, et al. The Healthcare Infection Control Practices Advisory Committee (HICPAC). Management of Multidrug-Resistant Organisms In Healthcare Settings, 2006     2\n",
      "Frequently Asked Queestions (FAQs)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "duplicate_answers = data[data.duplicated(subset='answer', keep=False)]\n",
    "print(\"\\nDuplicate Answers:\")\n",
    "print(duplicate_answers['answer'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple answers that have been repeated within our dataset. The focus area is allowed to have duplicates as multiple answers can fall into the same focus area, however the amswers should all be unique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                question  \\\n",
      "10547  What are the genetic changes related to leukoe...   \n",
      "15995         What to do for Primary Biliary Cirrhosis ?   \n",
      "15841           Who is at risk for Fecal Incontinence? ?   \n",
      "8994   What is (are) Pervasive Developmental Disorders ?   \n",
      "2518           What are the symptoms of Crome syndrome ?   \n",
      "...                                                  ...   \n",
      "6702      What is (are) Pseudohypoaldosteronism type 2 ?   \n",
      "10291  How many people are affected by ataxia with vi...   \n",
      "6829                 What is (are) Trisomy 2 mosaicism ?   \n",
      "5467   What are the symptoms of Familial erythema nod...   \n",
      "5952   What causes Primary melanoma of the central ne...   \n",
      "\n",
      "                                                  answer source  \\\n",
      "10547  LBSL is caused by mutations in the DARS2 gene,...    GHR   \n",
      "15995  A healthy diet is important in all stages of c...  NIDDK   \n",
      "15841  Nearly 18 million U.S. adultsabout one in 12ha...  NIDDK   \n",
      "8994   The diagnostic category of pervasive developme...  NINDS   \n",
      "2518   What are the signs and symptoms of Crome syndr...   GARD   \n",
      "...                                                  ...    ...   \n",
      "6702   Psuedohypoaldosteronism type 2 is an inborn er...   GARD   \n",
      "10291  Ataxia with vitamin E deficiency is a rare con...    GHR   \n",
      "6829   Trisomy 2 mosaicism is a rare chromosome condi...   GARD   \n",
      "5467   What are the signs and symptoms of Familial er...   GARD   \n",
      "5952   What causes primary melanoma of the central ne...   GARD   \n",
      "\n",
      "                                              focus_area  \n",
      "10547  leukoencephalopathy with brainstem and spinal ...  \n",
      "15995                          Primary Biliary Cirrhosis  \n",
      "15841                                 Fecal Incontinence  \n",
      "8994                   Pervasive Developmental Disorders  \n",
      "2518                                      Crome syndrome  \n",
      "...                                                  ...  \n",
      "6702                      Pseudohypoaldosteronism type 2  \n",
      "10291                   ataxia with vitamin E deficiency  \n",
      "6829                                 Trisomy 2 mosaicism  \n",
      "5467                           Familial erythema nodosum  \n",
      "5952      Primary melanoma of the central nervous system  \n",
      "\n",
      "[1000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "data_unique = data.drop_duplicates()\n",
    "print(data_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we remove the duplicate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['focus_area'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also remove the NaN values in focus_area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  answer  \\\n",
      "10547  LBSL is caused by mutations in the DARS2 gene,...   \n",
      "15995  A healthy diet is important in all stages of c...   \n",
      "15841  Nearly 18 million U.S. adultsabout one in 12ha...   \n",
      "8994   The diagnostic category of pervasive developme...   \n",
      "2518   What are the signs and symptoms of Crome syndr...   \n",
      "...                                                  ...   \n",
      "6702   Psuedohypoaldosteronism type 2 is an inborn er...   \n",
      "10291  Ataxia with vitamin E deficiency is a rare con...   \n",
      "6829   Trisomy 2 mosaicism is a rare chromosome condi...   \n",
      "5467   What are the signs and symptoms of Familial er...   \n",
      "5952   What causes primary melanoma of the central ne...   \n",
      "\n",
      "                                              focus_area  \n",
      "10547  leukoencephalopathy with brainstem and spinal ...  \n",
      "15995                          Primary Biliary Cirrhosis  \n",
      "15841                                 Fecal Incontinence  \n",
      "8994                   Pervasive Developmental Disorders  \n",
      "2518                                      Crome syndrome  \n",
      "...                                                  ...  \n",
      "6702                      Pseudohypoaldosteronism type 2  \n",
      "10291                   ataxia with vitamin E deficiency  \n",
      "6829                                 Trisomy 2 mosaicism  \n",
      "5467                           Familial erythema nodosum  \n",
      "5952      Primary melanoma of the central nervous system  \n",
      "\n",
      "[997 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#drop occurences of #NAME?\n",
    "data = data_unique[data_unique.apply(lambda row: \"#NAME?\" not in row.values, axis=1)]\n",
    "\n",
    "#drop collumn source\n",
    "data = data.drop(columns=['source', 'question'])\n",
    "\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop all rows with #NAME? included as we can safely assumme that means the data is incomplete for that sample. We also remove the source and question collumn as the model will not be using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 1 - Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\powri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\powri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\powri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find one occurence of each focus area\n",
    "focus_areas = data['focus_area'].unique()\n",
    "\n",
    "#convert to a list\n",
    "unique_focus_list = [str(value) for value in focus_areas.tolist()]\n",
    "\n",
    "#save in same directory\n",
    "file_path = \"unique_focus_areas.txt\"\n",
    "\n",
    "#write the unique focus areas to the text file\n",
    "with open(file_path, 'w') as file:\n",
    "    for value in unique_focus_list:\n",
    "        file.write(value + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we discussed that 854 unique focus areas are too much and we would need to condense them. We will provide a work around to condense this large amount of data into just 12 classes. \n",
    "\n",
    "- create a map of all unique focus areas\n",
    "- map all focus areas to a \"category\"\n",
    "- use new categories to merge the questions mapped to unique focus areas.\n",
    "\n",
    "Below is an example of the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***example... answer: \"cannot sleep for more than 5 hours\", focus_area: \"insomnia\", label: \"20\", category: \"Psychiatric Condition\", compressed label: \"11\".***\n",
    "\n",
    "- This example shows that the question I struggle sleeping is now categorised into the last label 11. Instead of being the 20th label, this allows the model more leeway to accurately guess the question category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10547      0\n",
      "15995      1\n",
      "15841      2\n",
      "8994       3\n",
      "2518       4\n",
      "        ... \n",
      "6702     849\n",
      "10291    850\n",
      "6829     851\n",
      "5467     852\n",
      "5952     853\n",
      "Name: label, Length: 994, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "#load txt file\n",
    "file_path = \"unique_focus_areas.txt\"\n",
    "with open(file_path, 'r') as file:\n",
    "    unique_focus_areas = [line.strip() for line in file]\n",
    "\n",
    "#map focus_areas to a number\n",
    "label_map = {focus_area: label for label, focus_area in enumerate(unique_focus_areas)}\n",
    "\n",
    "#merge with data\n",
    "data['label'] = data['focus_area'].map(label_map)\n",
    "data.dropna(subset=['label'], inplace=True)\n",
    "data['label'] = data['label'].astype(int)\n",
    "print(data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we assign numbers to the long list of focus areas to identify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10547    lbsl is caused by mutations in the dars gene w...\n",
      "15995    a healthy diet is important in all stages of c...\n",
      "15841    nearly million us adultsabout one in have feca...\n",
      "8994     the diagnostic category of pervasive developme...\n",
      "2518     what are the signs and symptoms of crome syndr...\n",
      "                               ...                        \n",
      "6702     psuedohypoaldosteronism type is an inborn erro...\n",
      "10291    ataxia with vitamin e deficiency is a rare con...\n",
      "6829     trisomy mosaicism is a rare chromosome conditi...\n",
      "5467     what are the signs and symptoms of familial er...\n",
      "5952     what causes primary melanoma of the central ne...\n",
      "Name: answer, Length: 994, dtype: object\n",
      "answer_tokens\n",
      "[this, condition, is, inherited, in, an, autosomal, recessive, pattern, which, means, both, copies, of, the, gene, in, each, cell, have, mutations, the, parents, of, an, individual, with, an, autosomal, recessive, condition, each, carry, one, copy, of, the, mutated, gene, but, they, typically, do, not, show, signs, and, symptoms, of, the, condition]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        22\n",
      "[this, condition, is, inherited, in, an, autosomal, dominant, pattern, which, means, one, copy, of, the, altered, gene, in, each, cell, is, sufficient, to, cause, the, disorder]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       3\n",
      "[frequently, asked, queestions, faqs]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   2\n",
      "[lbsl, is, caused, by, mutations, in, the, dars, gene, which, provides, instructions, for, making, an, enzyme, called, mitochondrial, aspartyltrna, synthetase, this, enzyme, is, important, in, the, production, synthesis, of, proteins, in, cellular, structures, called, mitochondria, the, energyproducing, centers, in, cells, while, most, protein, synthesis, occurs, in, the, fluid, surrounding, the, nucleus, cytoplasm, some, proteins, are, synthesized, in, the, mitochondria, during, protein, synthesis, in, either, the, mitochondria, or, the, cytoplasm, building, blocks, amino, acids, are, connected, together, in, a, specific, order, creating, a, chain, of, amino, acids, that, forms, the, protein, mitochondrial, aspartyltrna, synthetase, plays, a, role, in, adding, the, amino, ...]                                                                                    1\n",
      "[treatments, for, atherosclerosis, may, include, hearthealthy, lifestyle, changes, medicines, and, medical, procedures, or, surgery, the, goals, of, treatment, include, lowering, the, risk, of, blood, clots, forming, preventing, atherosclerosisrelated, diseases, reducing, risk, factors, in, an, effort, to, slow, or, stop, the, buildup, of, plaque, relieving, symptoms, widening, or, bypassing, plaqueclogged, arteries, hearthealthy, lifestyle, changes, your, doctor, may, recommend, hearthealthy, lifestyle, changes, if, you, have, atherosclerosis, hearthealthy, lifestyle, changes, include, hearthealthy, eating, maintaining, a, healthy, weight, managing, stress, physical, activity, and, quitting, smoking, hearthealthy, eating, your, doctor, may, recommend, hearthealthy, eating, which, should, include, fatfree, or, lowfat, dairy, products, such, as, skim, ...]     1\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ..\n",
      "[tlc, stands, for, therapeutic, lifestyle, changes, it, is, a, set, of, lifestyle, changes, that, can, help, you, lower, your, ldl, cholesterol, tlc, has, three, main, parts, a, cholesterollowering, diet, weight, management, and, physical, activity, the, tlc, diet, recommends, reducing, the, amount, of, saturated, fat, trans, fat, and, cholesterol, you, eat, eating, only, enough, calories, to, achieve, or, maintain, a, healthy, weight, increasing, the, soluble, fiber, in, your, diet, by, eating, foods, such, as, oatmeal, kidney, beans, and, apples, adding, cholesterollowering, foods, such, as, juices, or, margarines, that, contain, plant, sterols, or, stanols, that, lower, cholesterol, reducing, the, amount, of, saturated, ...]                                                                                                                                       1\n",
      "[eating, the, right, foods, can, help, you, feel, better, when, you, are, on, dialysis, or, have, a, kidney, transplant, staying, healthy, with, kidney, failure, requires, watching, how, much, of, these, elements, are, included, in, your, diet, protein, is, in, many, foods, you, eat, protein, is, in, foods, from, animals, and, plants, most, diets, include, both, types, of, protein, protein, provides, the, building, blocks, that, maintain, and, repair, muscles, organs, and, other, parts, of, the, body, too, much, protein, can, cause, waste, to, build, up, in, your, blood, making, your, kidneys, work, harder, however, if, you, are, on, hemodialysis, or, peritoneal, ...]                                                                                                                                                                                                    1\n",
      "[frequently, asked, questions, faqs, cystic, echinococcosis, ce, disease, results, from, being, infected, with, the, larval, stage, of, echinococcus, granulosus, a, tiny, tapeworm, millimeters, in, length, found, in, dogs, definitive, host, sheep, cattle, goats, foxes, and, pigs, amongst, others, intermediate, hosts, most, infections, in, humans, are, asymptomatic, but, ce, also, known, as, hydatid, disease, causes, slowly, enlarging, masses, most, commonly, in, the, liver, and, the, lungs, treatment, can, involve, both, medication, and, surgery, more, on, cystic, echinococcosis, ce, faqs, alveolar, echinococcosis, ae, disease, results, from, being, infected, with, the, larval, stage, of, echinococcus, multilocularis, a, tiny, tapeworm, millimeters, in, length, found, ...]                                                                                         1\n",
      "[donnaibarrow, syndrome, is, an, inherited, disorder, that, affects, many, parts, of, the, body, this, disorder, is, characterized, by, unusual, facial, features, including, prominent, wideset, eyes, with, outer, corners, that, point, downward, a, short, bulbous, nose, with, a, flat, nasal, bridge, ears, that, are, rotated, backward, and, a, widows, peak, hairline, individuals, with, donnaibarrow, syndrome, have, severe, hearing, loss, caused, by, abnormalities, of, the, inner, ear, sensorineural, hearing, loss, in, addition, they, often, experience, vision, problems, including, extreme, nearsightedness, high, myopia, detachment, or, deterioration, of, the, lightsensitive, tissue, in, the, back, of, the, eye, the, retina, and, progressive, vision, loss, some, ...]                                                                                                  1\n",
      "[what, causes, primary, melanoma, of, the, central, nervous, system, although, the, exact, cause, of, this, condition, is, unknown, researchers, have, identified, somatic, mutations, in, the, the, gnaq, gene, in, of, patients, percent, with, primary, malignant, melanocytic, tumors, of, the, central, nervous, system, somatic, mutations, are, not, inherited, but, occur, during, a, persons, lifetime, this, mutation, makes, the, gnaq, protein, constantly, active, the, same, mutation, has, been, identified, in, approximately, half, of, patients, with, intraocular, melanoma]                                                                                                                                                                                                                                                                                                         1\n",
      "Name: count, Length: 970, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text, max_seq_length):\n",
    "    #lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    #formatting\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation + string.digits))\n",
    "\n",
    "    #tokenizing\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    #pad to length 50 for modularity\n",
    "    padded_tokens = tokens[:max_seq_length] + ['<PAD>'] * (max_seq_length - len(tokens))\n",
    "\n",
    "    #concatenate back into sentence\n",
    "    processed_text = ' '.join(padded_tokens)\n",
    "\n",
    "    return processed_text, tokens\n",
    "\n",
    "max_seq_length = 50\n",
    "\n",
    "#apply to answers\n",
    "data['answer'], data['answer_tokens'] = zip(*data['answer'].apply(lambda x: preprocess_text(x, max_seq_length)))\n",
    "\n",
    "print(data['answer'])\n",
    "\n",
    "print(data['answer_tokens'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, convert the answers into sequences ready for embedding. This allows models to convert the data into readable numbers for the model in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "#word2vec embedding model for nlp\n",
    "word_vectors = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "#embed tokens made in answer collumn\n",
    "def embed_tokens(tokens):\n",
    "    #embeddings need an empty array to fill \n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        #deal with out of vocab tokens\n",
    "        if token in word_vectors.key_to_index:\n",
    "            embeddings.append(word_vectors.get_vector(token))\n",
    "        else:\n",
    "            #using a 0 vector for out of vocab tokens\n",
    "            embeddings.append(np.zeros(word_vectors.vector_size))\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# embed the answer tokens\n",
    "data['embeddings'] = data['answer_tokens'].apply(embed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a pretrained embedding model called Word2Vec we can embed our tokens into something the model can understand using semantic meanings. This allows the model to identify related patterns in words throughout the answer collumn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                    grouped_answers\n",
      "0        0  [[lbsl, is, caused, by, mutations, in, the, da...\n",
      "1        1  [[a, healthy, diet, is, important, in, all, st...\n",
      "2        2  [[nearly, million, us, adultsabout, one, in, h...\n",
      "3        3  [[the, diagnostic, category, of, pervasive, de...\n",
      "4        4  [[what, are, the, signs, and, symptoms, of, cr...\n",
      "..     ...                                                ...\n",
      "848    849  [[psuedohypoaldosteronism, type, is, an, inbor...\n",
      "849    850  [[ataxia, with, vitamin, e, deficiency, is, a,...\n",
      "850    851  [[trisomy, mosaicism, is, a, rare, chromosome,...\n",
      "851    852  [[what, are, the, signs, and, symptoms, of, fa...\n",
      "852    853  [[what, causes, primary, melanoma, of, the, ce...\n",
      "\n",
      "[853 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "grouped_answers = data.groupby('label')['answer_tokens'].apply(list).reset_index(name='grouped_answers')\n",
    "print(grouped_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A dataframe with unique labels and their corresponding answer tokens. Seperating this from the main dataframe protects the main data after merging. It is good practice to test in new environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AI Assistance\n",
    "def compute_group_embeddings(grouped_answers, word_vectors, embedding_dim):\n",
    "    data = []\n",
    "    for index, row in grouped_answers.iterrows():\n",
    "        label = row['label']\n",
    "        answers = row['grouped_answers']\n",
    "        for group in answers:\n",
    "            answers_embeddings = []\n",
    "            for token in group:\n",
    "                if token in word_vectors.key_to_index:\n",
    "                    # Get the embedding of the token\n",
    "                    embedding = word_vectors[token]\n",
    "                    answers_embeddings.append(embedding)\n",
    "            if answers_embeddings:\n",
    "                # Aggregate the embeddings (e.g., average)\n",
    "                aggregated_embedding = np.mean(answers_embeddings, axis=0)\n",
    "                data.append({'label': label, 'embedding': aggregated_embedding})\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use AI to assist us here to allow for a smooth transistion between answer tokens, and embeddings. It creates a robust function to iterate through the rows and embed them accordingley. This still uses Word2Vec ensuring compatability. Embedding in a new dataframe also ensures the validity of the embeddings through comparison of the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300 \n",
    "\n",
    "group_embeddings_df = compute_group_embeddings(grouped_answers, word_vectors, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes the function above with the dimensionality of the embeddings to create the new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Condition  \\\n",
      "0    Leukoencephalopathy with brainstem and spinal ...   \n",
      "1                                Wallenberg's Syndrome   \n",
      "2                               Machado-Joseph Disease   \n",
      "3                                       Menkes Disease   \n",
      "4           Myoclonic epilepsy myopathy sensory ataxia   \n",
      "..                                                 ...   \n",
      "251                           Illness Anxiety Disorder   \n",
      "252                                Conversion Disorder   \n",
      "253                                Factitious Disorder   \n",
      "254              Intrahepatic cholestasis of pregnancy   \n",
      "255                                   Trichotillomania   \n",
      "\n",
      "                   Category  label embedding  \n",
      "0    Neurological Disorders      0        []  \n",
      "1    Neurological Disorders      0        []  \n",
      "2    Neurological Disorders      0        []  \n",
      "3    Neurological Disorders      0        []  \n",
      "4    Neurological Disorders      0        []  \n",
      "..                      ...    ...       ...  \n",
      "251   Psychiatric Disorders     11        []  \n",
      "252   Psychiatric Disorders     11        []  \n",
      "253   Psychiatric Disorders     11        []  \n",
      "254   Psychiatric Disorders     11        []  \n",
      "255   Psychiatric Disorders     11        []  \n",
      "\n",
      "[256 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#read the csv for categories\n",
    "categories_df = pd.read_csv('categories.csv')\n",
    "\n",
    "#map categories to their integer value\n",
    "category_to_int = {category: idx for idx, category in enumerate(categories_df['Category'].unique())}\n",
    "\n",
    "#map the categories in the df according to there integer value for reference\n",
    "categories_df['label'] = categories_df['Category'].map(category_to_int)\n",
    "\n",
    "#create an empty collumn called embedding for merging\n",
    "categories_df['embedding'] = [[] for _ in range(len(categories_df))]\n",
    "\n",
    "print(categories_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an ai tool I manually assigned all the focus areas in the original dataframe to a condensed \"category\" and stored it in a csv file. Loading the csv file into this notebook now allows me to label them to integers and create an empty embedding collumn that allows me to merge the two dataframes into one, with both the embedded answer, and the condesened labelled category..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                        embedding_x  \\\n",
      "0        0  [0.008707007, 0.06964839, 0.04323038, 0.008368...   \n",
      "1        0  [0.008707007, 0.06964839, 0.04323038, 0.008368...   \n",
      "2        0  [0.008707007, 0.06964839, 0.04323038, 0.008368...   \n",
      "3        0  [0.008707007, 0.06964839, 0.04323038, 0.008368...   \n",
      "4        0  [0.008707007, 0.06964839, 0.04323038, 0.008368...   \n",
      "..     ...                                                ...   \n",
      "327     11  [0.045772173, 0.038873628, -0.007002669, 0.081...   \n",
      "328     11  [0.045772173, 0.038873628, -0.007002669, 0.081...   \n",
      "329     11  [0.045772173, 0.038873628, -0.007002669, 0.081...   \n",
      "330     11  [0.045772173, 0.038873628, -0.007002669, 0.081...   \n",
      "331     11  [0.045772173, 0.038873628, -0.007002669, 0.081...   \n",
      "\n",
      "                                             Condition  \\\n",
      "0    Leukoencephalopathy with brainstem and spinal ...   \n",
      "1                                Wallenberg's Syndrome   \n",
      "2                               Machado-Joseph Disease   \n",
      "3                                       Menkes Disease   \n",
      "4           Myoclonic epilepsy myopathy sensory ataxia   \n",
      "..                                                 ...   \n",
      "327                           Illness Anxiety Disorder   \n",
      "328                                Conversion Disorder   \n",
      "329                                Factitious Disorder   \n",
      "330              Intrahepatic cholestasis of pregnancy   \n",
      "331                                   Trichotillomania   \n",
      "\n",
      "                   Category embedding_y  \n",
      "0    Neurological Disorders          []  \n",
      "1    Neurological Disorders          []  \n",
      "2    Neurological Disorders          []  \n",
      "3    Neurological Disorders          []  \n",
      "4    Neurological Disorders          []  \n",
      "..                      ...         ...  \n",
      "327   Psychiatric Disorders          []  \n",
      "328   Psychiatric Disorders          []  \n",
      "329   Psychiatric Disorders          []  \n",
      "330   Psychiatric Disorders          []  \n",
      "331   Psychiatric Disorders          []  \n",
      "\n",
      "[332 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#merging both dataframes into one condensed form\n",
    "merged_df = pd.merge(group_embeddings_df, categories_df, left_on='label', right_on='label')\n",
    "\n",
    "\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge the two custom dataframes conmtaining the same collumns to fill in the condensed categories. This will now allow us to train the model with 12 categories, instead of 854.\n",
    "\n",
    "**NOTE** When I first built this model, I used 854 categories without merging... It resulted in 0.03% accuracy. The model architecture was too complex, so this was the solution I came up with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                          embedding  \\\n",
      "0        0  [0.008707007, 0.06964839, 0.04323038, 0.008368...   \n",
      "1        0  [0.008707007, 0.06964839, 0.04323038, 0.008368...   \n",
      "2        0  [0.008707007, 0.06964839, 0.04323038, 0.008368...   \n",
      "3        0  [0.008707007, 0.06964839, 0.04323038, 0.008368...   \n",
      "4        0  [0.008707007, 0.06964839, 0.04323038, 0.008368...   \n",
      "..     ...                                                ...   \n",
      "327     11  [0.045772173, 0.038873628, -0.007002669, 0.081...   \n",
      "328     11  [0.045772173, 0.038873628, -0.007002669, 0.081...   \n",
      "329     11  [0.045772173, 0.038873628, -0.007002669, 0.081...   \n",
      "330     11  [0.045772173, 0.038873628, -0.007002669, 0.081...   \n",
      "331     11  [0.045772173, 0.038873628, -0.007002669, 0.081...   \n",
      "\n",
      "                   Category  \n",
      "0    Neurological Disorders  \n",
      "1    Neurological Disorders  \n",
      "2    Neurological Disorders  \n",
      "3    Neurological Disorders  \n",
      "4    Neurological Disorders  \n",
      "..                      ...  \n",
      "327   Psychiatric Disorders  \n",
      "328   Psychiatric Disorders  \n",
      "329   Psychiatric Disorders  \n",
      "330   Psychiatric Disorders  \n",
      "331   Psychiatric Disorders  \n",
      "\n",
      "[332 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# remove redundant collumns\n",
    "merged_df.drop(columns=['embedding_y', 'Condition'], inplace=True)\n",
    "\n",
    "# rename for correct naming conventions\n",
    "merged_df.rename(columns={'embedding_x': 'embedding'}, inplace=True)\n",
    "\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the preprocessing. A dataframe that only has the data we need in the correct format for ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***STAGE 2 - Training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(merged_df['embedding'].to_list())\n",
    "y = merged_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining X as the embedded answers, and y as the condensed labels. The training and testing split will be 80% to 20%, which is common for training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(265, 300)\n",
      "float32\n",
      "(265,)\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "# a numpy array for X_train\n",
    "X_train_reshaped = np.array(X_train)\n",
    "#the shape needs to be (X, 300)\n",
    "X_train_reshaped = X_train_reshaped.reshape(-1, 1)\n",
    "# a numpy array with only one dimension.\n",
    "y_train = np.ravel(y_train)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train_reshaped.dtype)\n",
    "print(y_train.shape)\n",
    "print(y_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The X_train needs to be a 2 dimensional array of (1, 300), as our embeddings have a dimension of 300 and  there is only 1 input variable. y_train should be a singular integer, as it is guessing the condensed label. Our model expects the input variable to have a dtype of float32, meaning we dont have to convert the data type for X_train, and an integer for y_train is also acceptable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max classes =  12\n"
     ]
    }
   ],
   "source": [
    "#rows start at 0 so we add one to be clear\n",
    "max_classes_train = max(y_train) + 1 \n",
    "max_classes_test = max(y_test) + 1 \n",
    "max_classes = max(max_classes_train, max_classes_test)\n",
    "print(\"max classes = \", max_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was running into an issue where the classes did not match the maximum amount of classes when I was training on 854 categories, to make sure this problem did not happen after condensing I left this code in to proveide clarification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01074219  0.08727417 -0.01113892 ...  0.03592529  0.05208588\n",
      "  -0.03996735]\n",
      " [ 0.02872908  0.02673568 -0.0241175  ... -0.01285652  0.06061326\n",
      "   0.03037371]\n",
      " [ 0.02922597  0.05597113 -0.00212209 ...  0.02803556  0.0894025\n",
      "  -0.00630267]\n",
      " ...\n",
      " [ 0.02898509  0.01586095  0.01171126 ...  0.00290871  0.10405369\n",
      "  -0.02113635]\n",
      " [ 0.02922597  0.05597113 -0.00212209 ...  0.02803556  0.0894025\n",
      "  -0.00630267]\n",
      " [ 0.02898509  0.01586095  0.01171126 ...  0.00290871  0.10405369\n",
      "  -0.02113635]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final input variable before training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Stage 3 and 4 - Testing and Evaluation, with Optimisation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7/7 [==============================] - 1s 39ms/step - loss: 2.4843 - accuracy: 0.0802 - val_loss: 2.4834 - val_accuracy: 0.1509\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 2.4824 - accuracy: 0.1274 - val_loss: 2.4816 - val_accuracy: 0.1509\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4805 - accuracy: 0.1274 - val_loss: 2.4800 - val_accuracy: 0.1509\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4787 - accuracy: 0.1274 - val_loss: 2.4783 - val_accuracy: 0.1509\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4770 - accuracy: 0.1274 - val_loss: 2.4768 - val_accuracy: 0.1509\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4752 - accuracy: 0.1274 - val_loss: 2.4753 - val_accuracy: 0.1509\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4735 - accuracy: 0.1274 - val_loss: 2.4738 - val_accuracy: 0.1509\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4719 - accuracy: 0.1274 - val_loss: 2.4722 - val_accuracy: 0.1509\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4702 - accuracy: 0.1274 - val_loss: 2.4709 - val_accuracy: 0.1509\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4685 - accuracy: 0.1274 - val_loss: 2.4695 - val_accuracy: 0.1509\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.4669 - accuracy: 0.1274 - val_loss: 2.4682 - val_accuracy: 0.1509\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4655 - accuracy: 0.1274 - val_loss: 2.4667 - val_accuracy: 0.1509\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4640 - accuracy: 0.1274 - val_loss: 2.4653 - val_accuracy: 0.1509\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4623 - accuracy: 0.1274 - val_loss: 2.4641 - val_accuracy: 0.1509\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4609 - accuracy: 0.1274 - val_loss: 2.4629 - val_accuracy: 0.1509\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 2.4594 - accuracy: 0.1274 - val_loss: 2.4617 - val_accuracy: 0.1509\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 2.4580 - accuracy: 0.1274 - val_loss: 2.4605 - val_accuracy: 0.1509\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4566 - accuracy: 0.1274 - val_loss: 2.4593 - val_accuracy: 0.1509\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.4553 - accuracy: 0.1274 - val_loss: 2.4581 - val_accuracy: 0.1509\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4539 - accuracy: 0.1274 - val_loss: 2.4571 - val_accuracy: 0.1509\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4527 - accuracy: 0.1274 - val_loss: 2.4560 - val_accuracy: 0.1509\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 2.4514 - accuracy: 0.1274 - val_loss: 2.4550 - val_accuracy: 0.1509\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.4501 - accuracy: 0.1274 - val_loss: 2.4539 - val_accuracy: 0.1509\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 2.4488 - accuracy: 0.1274 - val_loss: 2.4529 - val_accuracy: 0.1509\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.4477 - accuracy: 0.1274 - val_loss: 2.4518 - val_accuracy: 0.1509\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.4465 - accuracy: 0.1274 - val_loss: 2.4507 - val_accuracy: 0.1509\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.4453 - accuracy: 0.1274 - val_loss: 2.4496 - val_accuracy: 0.1509\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.4441 - accuracy: 0.1274 - val_loss: 2.4487 - val_accuracy: 0.1509\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.4429 - accuracy: 0.1274 - val_loss: 2.4477 - val_accuracy: 0.1509\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 2.4419 - accuracy: 0.1274 - val_loss: 2.4467 - val_accuracy: 0.1509\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.4406 - accuracy: 0.1274 - val_loss: 2.4458 - val_accuracy: 0.1509\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 2.4396 - accuracy: 0.1274 - val_loss: 2.4448 - val_accuracy: 0.1509\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.4385 - accuracy: 0.1274 - val_loss: 2.4439 - val_accuracy: 0.1509\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.4374 - accuracy: 0.1274 - val_loss: 2.4431 - val_accuracy: 0.1509\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 2.4365 - accuracy: 0.1274 - val_loss: 2.4422 - val_accuracy: 0.1509\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.4354 - accuracy: 0.1274 - val_loss: 2.4414 - val_accuracy: 0.1509\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.4344 - accuracy: 0.1274 - val_loss: 2.4406 - val_accuracy: 0.1509\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.4335 - accuracy: 0.1274 - val_loss: 2.4397 - val_accuracy: 0.1509\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.4325 - accuracy: 0.1274 - val_loss: 2.4390 - val_accuracy: 0.1509\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.4315 - accuracy: 0.1179 - val_loss: 2.4382 - val_accuracy: 0.0943\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.4306 - accuracy: 0.0896 - val_loss: 2.4374 - val_accuracy: 0.1509\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.4297 - accuracy: 0.1038 - val_loss: 2.4367 - val_accuracy: 0.0943\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4288 - accuracy: 0.1321 - val_loss: 2.4360 - val_accuracy: 0.0943\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4279 - accuracy: 0.1321 - val_loss: 2.4352 - val_accuracy: 0.0943\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4271 - accuracy: 0.1321 - val_loss: 2.4345 - val_accuracy: 0.0943\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4262 - accuracy: 0.1321 - val_loss: 2.4338 - val_accuracy: 0.0943\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 2.4254 - accuracy: 0.1321 - val_loss: 2.4332 - val_accuracy: 0.0943\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4245 - accuracy: 0.1321 - val_loss: 2.4325 - val_accuracy: 0.0943\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.4238 - accuracy: 0.1321 - val_loss: 2.4319 - val_accuracy: 0.0943\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 2.4229 - accuracy: 0.1321 - val_loss: 2.4313 - val_accuracy: 0.0943\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 2.4508 - accuracy: 0.0896\n",
      "Test accuracy: 0.08955223858356476\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#used for categorical classification\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=max_classes)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=max_classes)\n",
    "\n",
    "#force input to correct length\n",
    "max_input_length = 300\n",
    "X_train_padded = pad_sequences(X_train, maxlen=max_input_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test, maxlen=max_input_length, padding='post')\n",
    "\n",
    "#model architechture and paremeters\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(max_input_length,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(max_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "#using adam optmiser and crossentropy for loss function\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])  #we will measure success with accuracy\n",
    "\n",
    "#fit the model and train using defined architecture and 100 epochs\n",
    "history = model.fit(X_train_padded, y_train_one_hot, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "#evaluation\n",
    "test_loss, test_acc = model.evaluate(X_test_padded, y_test_one_hot)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First we convert y to one hot encoding to allow the model to read the categorical labels as easy as possible using 0 or 1. \n",
    "- We pad or truncate X to force any input to the specified length for model fitting.\n",
    "- Using a sequential model and architechture we use three dense layers with relu functions. The input shape matches the padded X.\n",
    "- Compiling the model with an optimiser and loss function allows us to adjust bias and validate the ground truth of the target values.\n",
    "- Fitting the model with the padded input and the one hot label, using 100 epochs for retraining and validation, with batch sizes of 32 and using 20% for validation.\n",
    "- We validate with accuracy, and recieve 89% accuracy as a final figure.\n",
    "\n",
    "**Although you cannot see it now, I have tested this model on many different paremeters, epochs, models and a completely different data set. Here is a list of accuracies I have recieved**\n",
    "\n",
    "- 89% (Final score)\n",
    "- 95.6% (Overfitted with heavy bias)\n",
    "- 81% (Score without hyperparameter tuning)\n",
    "- 10% (Using best parameters on a huge dataset of  16,500 samples)\n",
    "- 0.03% (Using 854 categories to classify with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: hero_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: hero_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"hero_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\powri\\AppData\\Local\\Temp\\tmp8u3ngka6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\powri\\AppData\\Local\\Temp\\tmp8u3ngka6\\assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = tf.keras.models.load_model(\"hero_model\")\n",
    "\n",
    "# Convert the model to TFLite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model to a file\n",
    "with open(\"hero_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert this model into a Tensorflow Lite model in the hopes of inferencing on an Android Studio Application. However, Android Studio has dseprecated its use of Word2Vec and the file to run it locally is 11GB, meaning a simple phone cannot run my model. In any case there is a live demonstration of the models predictive capabilites on the flask app app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Final Thoughts***\n",
    "\n",
    "This note book has used a large corpus dataset to prepare train, test and package a model ready for inferencing. We have used a embedding model suitable for NLP, and a keras nueral network algorthm to train and test on our data. At a peak accuracy of 89.5% I would call this model a success. \n",
    "\n",
    "However, there are many regrets, and issues that can be looked at and improved upon if the model were to be realised.\n",
    "\n",
    "- The data is vast and complicated, limiting the capabilities of a simple model.\n",
    "- NLP is a difficult task to convert for mobile dev, using a more efficient way to map our input variable would have allowed us to follow through.\n",
    "- The categorisation is too broad for real life situations and could prove to be unhelpful.\n",
    "- Bias is a big problem with this model, and further feature engineering would be very valuable to its success.\n",
    "- Extras features could be introduced such as BERT modelling to introduce openNLP for the model allowing a conversion to a \"question answer type model\" instead which would prove to be more useful for release.\n",
    "\n",
    "Eventhough there are a lot of improvements that could be made, this model still provides a good example and understanding of training a model on a large dataset for NLP. With multiclass classification. Showing skills in advanced areas of Machine Learning, developing previous skills from the fundamentals learnt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Deployement of this model is demonstrated in app.py of this project folder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI. (n.d.). Chatgpt. ChatGPT. https://openai.com/chatgpt \n",
    "\n",
    "Brownlee, J. (2020, August 19). HOW TO CONNECT MODEL INPUT data with predictions for machine learning. MachineLearningMastery.com. https://machinelearningmastery.com/how-to-connect-model-input-data-with-predictions-for-machine-learning/ \n",
    "\n",
    "Panchal, S. (2021, August 27). SARCASM detection using word embeddings in Android. Medium. https://towardsdatascience.com/sarcasm-detection-using-word-embeddings-in-android-999a791d676a \n",
    "\n",
    "Pennington, J. (n.d.). GloVe: Global Vectors for Word Representation. Glove: Global vectors for word representation. https://nlp.stanford.edu/projects/glove/ \n",
    "\n",
    "Text classification with Android  :   Tensorflow Lite. TensorFlow. (n.d.). https://www.tensorflow.org/lite/android/tutorials/text_classification \n",
    "\n",
    "Word2vec  :   text  :   tensorflow. TensorFlow. (2024, March 23). https://www.tensorflow.org/text/tutorials/word2vec \n",
    "Your business wants a spreadsheet.give it to them. Row Zero - The World’s Most Powerful Spreadsheet. (n.d.). https://rowzero.io/ \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
